{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXl3H0Y3K3UvajvHpsrpB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vent0906/ww/blob/main/self_learn_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Summary: Learning Graph Convolutional Networks (GCN) with PyTorch Geometric\n",
        "\n",
        "In this project, I explored how to build and train a Graph Convolutional Network (GCN) using the PyTorch Geometric (PyG) framework. The main goal was to understand how GCNs operate on graph-structured data and apply them to a real-world graph classification problem.\n",
        "\n",
        "I used the **MUTAG** dataset, a well-known benchmark from the TUDataset collection, where each graph represents a molecule and the task is to classify whether it is mutagenic. I started by analyzing the dataset's structure — including the number of graphs, node features, edges, and class distribution — and verified key graph properties such as isolated nodes, self-loops, and graph directionality.\n",
        "\n",
        "I then implemented a **3-layer GCN** model with ReLU activations and global mean pooling for graph-level readout, followed by a fully connected classification layer. I defined training and testing procedures using batched data loaders and monitored performance using accuracy metrics.\n",
        "\n",
        "To evaluate model performance, I tracked both training and test accuracy across 170 epochs and visualized the results with Matplotlib. This allowed me to observe convergence behavior and check for potential overfitting.\n",
        "\n",
        "Overall, this hands-on experiment helped me understand the full GCN training pipeline, including dataset loading, model design, loss optimization, evaluation, and visualization in the context of graph classification.\n"
      ],
      "metadata": {
        "id": "BcAoPb_WHbm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iWfGnVThT8X",
        "outputId": "43bf3782-9e17-4f03-8c88-3508a446c80b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dZt1kWbbmnA",
        "outputId": "1003361d-8662-4e98-908e-2c78bb81a770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of graphs: 188\n",
            "Number of features: 7\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Average node degree: 2.24\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "# Load the MUTAG dataset from the TUDataset collection\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')              # Total number of graph samples in the dataset\n",
        "print(f'Number of features: {dataset.num_features}')    # Node feature dimension\n",
        "print(f'Number of classes: {dataset.num_classes}')      # Number of target classes for classification\n",
        "\n",
        "# Access the first graph object in the dataset\n",
        "data = dataset[0]\n",
        "\n",
        "print()\n",
        "print(data)  # Print full info of the graph object (e.g., edge_index, x, y)\n",
        "print('=============================================================')\n",
        "\n",
        "# Extract structural statistics of the graph\n",
        "print(f'Number of nodes: {data.num_nodes}')                     # Total number of nodes\n",
        "print(f'Number of edges: {data.num_edges}')                     # Total number of edges\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')  # Average degree = edges/nodes\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')       # Whether there are nodes with no connections\n",
        "print(f'Has self-loops: {data.has_self_loops()}')               # Whether the graph contains self-loops\n",
        "print(f'Is undirected: {data.is_undirected()}')                 # Whether the graph is undirected\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "# Shuffle the dataset to randomize the order of graphs\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_dataset = dataset[:150]  # First 150 graphs for training\n",
        "test_dataset = dataset[150:]   # Remaining graphs for testing\n",
        "\n",
        "# Print the size of each split\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pz9Z902ejK1",
        "outputId": "62b333db-f5d8-4a57-8a4f-a9be245d81e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Create a data loader for the training set with batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create a data loader for the test set without shuffling\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Jkp0UuQ2en58"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "\n",
        "        # Define 3 GCN layers\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Final linear classifier\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Step 1: Node feature transformation using GCN layers\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Step 2: Readout layer - aggregate node features into graph embeddings\n",
        "        x = global_mean_pool(x, batch)  # Shape: [num_graphs, hidden_channels]\n",
        "\n",
        "        # Step 3: Classification\n",
        "        x = F.dropout(x, p=0.5, training=self.training)  # Apply dropout during training\n",
        "        x = self.lin(x)  # Linear classification layer\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKQWqHk8erC0",
        "outputId": "8685f42a-2d17-47b8-d805-2a704ae3b2b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "\n",
        "# Adjust iframe height for better visualization in Google Colab\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# Initialize the GCN model, optimizer, and loss function\n",
        "model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for data in train_loader:  # Iterate over training batches\n",
        "        out = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
        "        loss = criterion(out, data.y)  # Compute cross-entropy loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model parameters\n",
        "        optimizer.zero_grad()  # Clear gradients for next step\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:  # Iterate over test batches\n",
        "        out = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
        "        pred = out.argmax(dim=1)  # Get predicted class with highest probability\n",
        "        correct += int((pred == data.y).sum())  # Count correct predictions\n",
        "    return correct / len(loader.dataset)  # Compute accuracy\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 171):  # Train for 170 epochs\n",
        "    train()\n",
        "    train_acc = test(train_loader)  # Evaluate on training set\n",
        "    test_acc = test(test_loader)    # Evaluate on test set\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "JGDyGyTJewbV",
        "outputId": "8396caee-b308-4ed4-be25-653e5dc6df7c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 008, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 012, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 013, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 014, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 015, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 016, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 017, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 018, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 019, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 020, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 021, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 022, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 023, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 024, Train Acc: 0.7267, Test Acc: 0.8421\n",
            "Epoch: 025, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 026, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 027, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 028, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 029, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 030, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 033, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 034, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 035, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 036, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 037, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 038, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 039, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 040, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 041, Train Acc: 0.7467, Test Acc: 0.7368\n",
            "Epoch: 042, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 043, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 044, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 046, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 047, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 048, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 049, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 050, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 051, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 052, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 053, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 054, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 055, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 056, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 057, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 058, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 059, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 060, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 061, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 062, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 063, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 064, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 065, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 066, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 067, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 068, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 069, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 070, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 071, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 072, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 073, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 075, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 076, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 077, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 078, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 079, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 080, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 081, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 082, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 083, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 084, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 085, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 086, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 087, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 088, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 089, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 090, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 091, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 092, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 093, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 094, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 095, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 096, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 097, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 098, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 099, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 100, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 101, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 102, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 103, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 104, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 105, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 106, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 107, Train Acc: 0.7733, Test Acc: 0.7105\n",
            "Epoch: 108, Train Acc: 0.8000, Test Acc: 0.7632\n",
            "Epoch: 109, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 110, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 111, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 112, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 113, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 115, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 116, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 117, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 118, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 119, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 120, Train Acc: 0.8000, Test Acc: 0.7105\n",
            "Epoch: 121, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 122, Train Acc: 0.7667, Test Acc: 0.7105\n",
            "Epoch: 123, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 124, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 125, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 126, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 127, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 128, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 129, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 130, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 131, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 132, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 133, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 134, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 135, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 136, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 137, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 138, Train Acc: 0.8133, Test Acc: 0.7105\n",
            "Epoch: 139, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 140, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 141, Train Acc: 0.8000, Test Acc: 0.6579\n",
            "Epoch: 142, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 143, Train Acc: 0.7933, Test Acc: 0.7632\n",
            "Epoch: 144, Train Acc: 0.7867, Test Acc: 0.7368\n",
            "Epoch: 145, Train Acc: 0.8267, Test Acc: 0.7368\n",
            "Epoch: 146, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 147, Train Acc: 0.7800, Test Acc: 0.7105\n",
            "Epoch: 148, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 149, Train Acc: 0.8200, Test Acc: 0.7105\n",
            "Epoch: 150, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 151, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 152, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 153, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 154, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 155, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 156, Train Acc: 0.7800, Test Acc: 0.7105\n",
            "Epoch: 157, Train Acc: 0.8000, Test Acc: 0.7368\n",
            "Epoch: 158, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 159, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 160, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 161, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 162, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 163, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 164, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 165, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 166, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 167, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 168, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 169, Train Acc: 0.8000, Test Acc: 0.7632\n",
            "Epoch: 170, Train Acc: 0.8000, Test Acc: 0.7632\n"
          ]
        }
      ]
    }
  ]
}