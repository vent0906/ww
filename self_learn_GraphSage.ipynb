{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJOGdqH0yDq3atTHb69KuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vent0906/ww/blob/main/self_learn_GraphSage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GraphSAGE Self-Study Summary\n",
        "\n",
        "I independently studied GraphSAGE for link prediction using the PyTorch Geometric (PyG) framework. Starting with the Cora citation dataset, I learned to preprocess the graph by splitting edges into training and testing sets, generating negative samples, and formatting them into PyG-compatible structures. I implemented a two-layer GraphSAGE model using mean aggregation, along with both dot product and MLP-based edge predictors. Through training with binary cross-entropy loss and evaluation using AUC scores, I developed a complete pipeline for supervised link prediction on graph data. This project deepened my understanding of graph neural networks, edge-level tasks, and PyG's modular design.\n"
      ],
      "metadata": {
        "id": "HVThnm91PX7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9czOwXakJ8vt",
        "outputId": "be6ac37d-70e6-4f92-d423-257b5db3489f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/cu118/repo.html"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IuiuaDtaId9z",
        "outputId": "36f47023-f46b-4052-cb28-99ed2dfb5e03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/cu118/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/cu118/dgl-2.1.0%2Bcu118-cp311-cp311-manylinux1_x86_64.whl (748.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m748.2/748.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.14.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (5.9.5)\n",
            "Collecting torchdata>=0.5.0 (from dgl)\n",
            "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2025.1.31)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata>=0.5.0->dgl) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (3.0.2)\n",
            "Downloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata, dgl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dgl-2.1.0+cu118 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchdata-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SLeSZ_6GGYUt"
      },
      "outputs": [],
      "source": [
        "# 导入相关的库\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "# Load the Cora citation network dataset from the Planetoid benchmark collection.\n",
        "# This dataset is widely used for semi-supervised node classification in GNN research.\n",
        "# The dataset will be downloaded and cached in the 'data/Planetoid' directory.\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
        "\n",
        "# Access the single graph instance within the Cora dataset.\n",
        "# Cora contains only one large citation graph.\n",
        "# The returned object contains node features, edge indices, labels, and train/val/test masks.\n",
        "data = dataset[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R32he0SYH1yz",
        "outputId": "8ff814b4-fc66-4af2-f1b8-4827c4170eb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_scipy_sparse_matrix\n",
        "# Extract edge index and convert to numpy arrays (u, v pairs)\n",
        "edge_index = data.edge_index\n",
        "u = edge_index[0].numpy()\n",
        "v = edge_index[1].numpy()\n",
        "\n",
        "# Generate shuffled edge IDs\n",
        "eids = np.arange(edge_index.size(1))\n",
        "np.random.shuffle(eids)\n",
        "\n",
        "# Split positive edges into training and testing sets (90/10 split)\n",
        "test_size = int(len(eids) * 0.1)\n",
        "train_size = len(eids) - test_size\n",
        "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
        "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
        "\n",
        "# Build adjacency matrix to find non-existent (negative) edges\n",
        "adj = to_scipy_sparse_matrix(edge_index, num_nodes=data.num_nodes).tocoo()\n",
        "adj_dense = adj.toarray()\n",
        "\n",
        "# Generate the negative edge candidates: 1 - A - I\n",
        "adj_neg = 1 - adj_dense - np.eye(data.num_nodes)\n",
        "neg_u, neg_v = np.where(adj_neg > 0)\n",
        "\n",
        "# Randomly sample the same number of negative edges as positive ones\n",
        "neg_eids = np.random.choice(len(neg_u), len(eids), replace=False)\n",
        "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
        "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
        "\n",
        "# Convert all edge sets to PyTorch edge_index format (shape [2, num_edges])\n",
        "train_pos_edge_index = torch.tensor([train_pos_u, train_pos_v], dtype=torch.long)\n",
        "train_neg_edge_index = torch.tensor([train_neg_u, train_neg_v], dtype=torch.long)\n",
        "test_pos_edge_index = torch.tensor([test_pos_u, test_pos_v], dtype=torch.long)\n",
        "test_neg_edge_index = torch.tensor([test_neg_u, test_neg_v], dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "WopIvMGLH6Hc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get the complete edge index from the original graph\n",
        "edge_index = data.edge_index  # shape: [2, num_edges]\n",
        "\n",
        "# Step 2: Generate a shuffled list of edge indices\n",
        "eids = np.arange(edge_index.size(1))\n",
        "np.random.shuffle(eids)\n",
        "\n",
        "# Step 3: Split into test edges (10%) and train edges (90%)\n",
        "test_size = int(len(eids) * 0.1)\n",
        "train_edges = eids[test_size:]  # 90% for training\n",
        "test_edges = eids[:test_size]   # 10% for testing\n",
        "\n",
        "# Step 4: Create a new edge_index tensor containing only the training edges\n",
        "train_edge_index = edge_index[:, train_edges]\n",
        "\n",
        "# Step 5: Deep-copy the original graph and replace edge_index with training-only edges\n",
        "train_data = deepcopy(data)\n",
        "train_data.edge_index = torch.tensor(train_edge_index, dtype=torch.long)\n",
        "\n",
        "# Now `train_data` is a graph with test edges removed,\n",
        "# which can be safely used for training the GNN without label leakage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WHoVOF3H7Fk",
        "outputId": "51b46388-7730-402e-f81e-72792148f3a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-30f97188ebfb>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_data.edge_index = torch.tensor(train_edge_index, dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "# Define a 2-layer GraphSAGE model using PyTorch Geometric\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        # First GraphSAGE layer: input -> hidden dimension\n",
        "        self.conv1 = SAGEConv(in_feats, hidden_feats, aggr='mean')  # mean aggregation\n",
        "        # Second GraphSAGE layer: hidden -> hidden dimension\n",
        "        self.conv2 = SAGEConv(hidden_feats, hidden_feats, aggr='mean')\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Forward pass of GraphSAGE model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Node feature matrix of shape [num_nodes, in_feats]\n",
        "            edge_index (LongTensor): Edge list in COO format [2, num_edges]\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Node embeddings of shape [num_nodes, hidden_feats]\n",
        "        \"\"\"\n",
        "        h = self.conv1(x, edge_index)  # Apply first SAGEConv\n",
        "        h = F.relu(h)                  # Apply non-linearity\n",
        "        h = self.conv2(h, edge_index)  # Apply second SAGEConv\n",
        "        return h  # Output final node embeddings"
      ],
      "metadata": {
        "id": "cfJbjk5QIDEz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert training positive and negative edge pairs into edge_index tensors.\n",
        "# Each edge_index is of shape [2, num_edges] in PyG.\n",
        "\n",
        "# Training positive edges: (u, v) → [2, num_train_pos_edges]\n",
        "train_pos_edge_index = torch.tensor([train_pos_u, train_pos_v], dtype=torch.long)\n",
        "\n",
        "# Training negative edges: (u, v) → [2, num_train_neg_edges]\n",
        "train_neg_edge_index = torch.tensor([train_neg_u, train_neg_v], dtype=torch.long)\n",
        "\n",
        "# Testing positive edges: used for AUC evaluation\n",
        "test_pos_edge_index = torch.tensor([test_pos_u, test_pos_v], dtype=torch.long)\n",
        "\n",
        "# Testing negative edges: used for AUC evaluation\n",
        "test_neg_edge_index = torch.tensor([test_neg_u, test_neg_v], dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55vrFlPGIEfK",
        "outputId": "c5f1504f-d16d-46b1-f411-92000fe9b17c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d080c9faff8e>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  train_pos_edge_index = torch.tensor([train_pos_u, train_pos_v], dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot Product Predictor: scores edges using the dot product of node embeddings\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, h, edge_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            h (Tensor): Node embeddings of shape [num_nodes, hidden_dim]\n",
        "            edge_index (LongTensor): Edge list of shape [2, num_edges]\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Edge scores of shape [num_edges], higher means more likely to exist\n",
        "        \"\"\"\n",
        "        src, dst = edge_index\n",
        "        # Element-wise product and sum → dot product\n",
        "        score = (h[src] * h[dst]).sum(dim=1)\n",
        "        return score\n",
        "\n",
        "\n",
        "# MLP-based Predictor: scores edges using a learned 2-layer feedforward network\n",
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def forward(self, h, edge_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            h (Tensor): Node embeddings of shape [num_nodes, hidden_dim]\n",
        "            edge_index (LongTensor): Edge list of shape [2, num_edges]\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Edge scores of shape [num_edges]\n",
        "        \"\"\"\n",
        "        src, dst = edge_index\n",
        "        edge_input = torch.cat([h[src], h[dst]], dim=1)  # concat source and destination\n",
        "        x = F.relu(self.W1(edge_input))\n",
        "        x = self.W2(x)\n",
        "        return x.squeeze(1)  # shape: [num_edges]\n",
        "\n"
      ],
      "metadata": {
        "id": "nmzIch--IH4a"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GraphSAGE model and predictor\n",
        "model = GraphSAGE(data.num_features, 16)  # GraphSAGE: input_dim → hidden_dim\n",
        "pred = DotPredictor()  # You can also use MLPPredictor(16)\n",
        "\n",
        "# Compute binary cross-entropy loss from positive and negative edge scores\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    \"\"\"\n",
        "    Compute binary classification loss for link prediction.\n",
        "\n",
        "    Args:\n",
        "        pos_score (Tensor): Scores for positive (real) edges\n",
        "        neg_score (Tensor): Scores for negative (fake) edges\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Binary cross-entropy loss\n",
        "    \"\"\"\n",
        "    scores = torch.cat([pos_score, neg_score], dim=0)\n",
        "    labels = torch.cat([\n",
        "        torch.ones(pos_score.size(0), device=pos_score.device),\n",
        "        torch.zeros(neg_score.size(0), device=neg_score.device)\n",
        "    ])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "# Compute AUC score for evaluation\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    \"\"\"\n",
        "    Compute ROC AUC score for link prediction.\n",
        "\n",
        "    Args:\n",
        "        pos_score (Tensor): Scores for positive edges\n",
        "        neg_score (Tensor): Scores for negative edges\n",
        "\n",
        "    Returns:\n",
        "        float: AUC score\n",
        "    \"\"\"\n",
        "    scores = torch.cat([pos_score, neg_score], dim=0).detach().cpu().numpy()\n",
        "    labels = torch.cat([\n",
        "        torch.ones(pos_score.size(0)),\n",
        "        torch.zeros(neg_score.size(0))\n",
        "    ]).cpu().numpy()\n",
        "    return roc_auc_score(labels, scores)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WUHR-38xIM5D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Initialize optimizer to jointly update the GNN model and the edge predictor\n",
        "# Using Adam optimizer with a learning rate of 0.01\n",
        "optimizer = torch.optim.Adam(\n",
        "    itertools.chain(model.parameters(), pred.parameters()),\n",
        "    lr=0.01\n",
        ")\n",
        "\n",
        "# 2. Training loop for link prediction\n",
        "for epoch in range(100):\n",
        "    model.train()  # Set model to training mode\n",
        "    pred.train()   # Set predictor (Dot or MLP) to training mode\n",
        "\n",
        "    # Step 1: Forward pass through GNN to compute node embeddings\n",
        "    h = model(data.x, data.edge_index)  # data.x: node features, data.edge_index: graph structure\n",
        "\n",
        "    # Step 2: Compute predicted scores for positive and negative training edges\n",
        "    pos_score = pred(h, train_pos_edge_index)  # Scores for edges that exist\n",
        "    neg_score = pred(h, train_neg_edge_index)  # Scores for sampled non-existing edges\n",
        "\n",
        "    # Step 3: Compute binary classification loss (1 for pos edges, 0 for neg edges)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "    # Step 4: Backward pass and optimizer update\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "    loss.backward()        # Compute gradients\n",
        "    optimizer.step()       # Update parameters\n",
        "\n",
        "    # Optional: Print loss every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 3. Evaluation phase — compute AUC on the test set\n",
        "model.eval()  # Set GNN model to evaluation mode\n",
        "pred.eval()   # Set predictor to evaluation mode\n",
        "\n",
        "with torch.no_grad():  # Disable gradient tracking for evaluation\n",
        "    h = model(data.x, data.edge_index)  # Compute node embeddings again\n",
        "\n",
        "    # Predict link scores for test set edges\n",
        "    pos_score = pred(h, test_pos_edge_index)\n",
        "    neg_score = pred(h, test_neg_edge_index)\n",
        "\n",
        "    # Compute ROC AUC score: how well the model separates positive from negative links\n",
        "    auc = compute_auc(pos_score, neg_score)\n",
        "    print(f\"Final Test AUC: {auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ-0_YnPINzT",
        "outputId": "f07f8588-c85c-4fcc-be8f-197e940cfe8c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000 | Loss: 0.7122\n",
            "Epoch 005 | Loss: 0.6723\n",
            "Epoch 010 | Loss: 0.5882\n",
            "Epoch 015 | Loss: 0.5398\n",
            "Epoch 020 | Loss: 0.5177\n",
            "Epoch 025 | Loss: 0.4903\n",
            "Epoch 030 | Loss: 0.4700\n",
            "Epoch 035 | Loss: 0.4499\n",
            "Epoch 040 | Loss: 0.4269\n",
            "Epoch 045 | Loss: 0.4055\n",
            "Epoch 050 | Loss: 0.3854\n",
            "Epoch 055 | Loss: 0.3651\n",
            "Epoch 060 | Loss: 0.3457\n",
            "Epoch 065 | Loss: 0.3263\n",
            "Epoch 070 | Loss: 0.3056\n",
            "Epoch 075 | Loss: 0.2833\n",
            "Epoch 080 | Loss: 0.2596\n",
            "Epoch 085 | Loss: 0.2345\n",
            "Epoch 090 | Loss: 0.2085\n",
            "Epoch 095 | Loss: 0.1853\n",
            "Final Test AUC: 0.8513\n"
          ]
        }
      ]
    }
  ]
}