{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vent0906/ww/blob/main/GAT_HAN_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5cc7df0",
      "metadata": {
        "id": "a5cc7df0"
      },
      "source": [
        "# Graph Attention Networks (GAT) and Heterogeneous Attention Networks (HAN) Tutorial\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Single-head and Multi-head GAT layer implementation\n",
        "2. Full GAT model on Cora dataset\n",
        "3. HAN model with node- and semantic-level attention on ACM dataset\n",
        "4. Training and visualization steps\n",
        "\n",
        "---\n",
        "\n",
        "**Please ensure the following packages are installed in your environment**:\n",
        "```bash\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html\n",
        "!pip install torchdata\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642d1f18",
      "metadata": {
        "id": "642d1f18"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn import GATConv\n",
        "\n",
        "# Single-head GAT Layer\n",
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.g = g\n",
        "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "\n",
        "    def message_func(self, edges):\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "    def reduce_func(self, nodes):\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}\n",
        "\n",
        "    def forward(self, h):\n",
        "        z = self.fc(h)\n",
        "        self.g.ndata['z'] = z\n",
        "        self.g.apply_edges(self.edge_attention)\n",
        "        self.g.update_all(self.message_func, self.reduce_func)\n",
        "        return self.g.ndata.pop('h')\n",
        "\n",
        "# Multi-head wrapper\n",
        "class MultiHeadGATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
        "        super(MultiHeadGATLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList([GATLayer(g, in_dim, out_dim) for _ in range(num_heads)])\n",
        "        self.merge = merge\n",
        "\n",
        "    def forward(self, h):\n",
        "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
        "        return torch.cat(head_outs, dim=1) if self.merge == 'cat' else torch.mean(torch.stack(head_outs), dim=0)\n",
        "\n",
        "# Full GAT model\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
        "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
        "\n",
        "    def forward(self, h):\n",
        "        h = self.layer1(h)\n",
        "        h = F.elu(h)\n",
        "        h = self.layer2(h)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd093e5",
      "metadata": {
        "id": "3bd093e5"
      },
      "outputs": [],
      "source": [
        "# Semantic-level attention\n",
        "class SemanticAttention(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size=128):\n",
        "        super(SemanticAttention, self).__init__()\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        w = self.project(z).mean(0)\n",
        "        beta = torch.softmax(w, dim=0)\n",
        "        beta = beta.expand((z.shape[0],) + beta.shape)\n",
        "        return (beta * z).sum(1)\n",
        "\n",
        "# HAN Layer\n",
        "class HANLayer(nn.Module):\n",
        "    def __init__(self, num_meta_paths, in_size, out_size, num_heads, dropout):\n",
        "        super(HANLayer, self).__init__()\n",
        "        self.gat_layers = nn.ModuleList([\n",
        "            GATConv(in_size, out_size, num_heads, dropout, dropout, activation=F.elu)\n",
        "            for _ in range(num_meta_paths)\n",
        "        ])\n",
        "        self.semantic_attention = SemanticAttention(out_size * num_heads)\n",
        "\n",
        "    def forward(self, gs, h):\n",
        "        semantic_embeddings = [gat(gs[i], h).flatten(1) for i, gat in enumerate(self.gat_layers)]\n",
        "        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)\n",
        "        return self.semantic_attention(semantic_embeddings)\n",
        "\n",
        "# Full HAN model\n",
        "class HAN(nn.Module):\n",
        "    def __init__(self, num_meta_paths, in_size, hidden_size, out_size, num_heads, dropout):\n",
        "        super(HAN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(HANLayer(num_meta_paths, in_size, hidden_size, num_heads[0], dropout))\n",
        "        for l in range(1, len(num_heads)):\n",
        "            self.layers.append(HANLayer(num_meta_paths, hidden_size * num_heads[l-1], hidden_size, num_heads[l], dropout))\n",
        "        self.predict = nn.Linear(hidden_size * num_heads[-1], out_size)\n",
        "\n",
        "    def forward(self, g, h, is_training=True):\n",
        "        for layer in self.layers:\n",
        "            h = layer(g, h)\n",
        "        return self.predict(h) if is_training else h\n"
      ]
    }
  ],
  "metadata": {
    "language": "python",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}